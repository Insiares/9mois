{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def process_table(engine, metadata, table_name):\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    with engine.connect() as connection:\n",
    "        result_set = connection.execute(table.select()).fetchall()\n",
    "    return result_set\n",
    "\n",
    "def concatenate_row_values(row):\n",
    "    return ''.join(str(value) for value in row if isinstance(value, str))    \n",
    "\n",
    "def get_document_id(doc):\n",
    "    return doc[0]\n",
    "\n",
    "def search(query, table_name= 'Toutes'):\n",
    "    if table_name != \"Toutes\":\n",
    "        documents = data[table_name]\n",
    "        vectorizer = vectorizers[table_name]\n",
    "        tfidf_matrix = vectorizer.transform([concatenate_row_values(row) for row in documents])\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "        ranked_scores = sorted([(score, row) for score, row in zip(scores, documents)], reverse=True, key=lambda x: x[0])\n",
    "        return ranked_scores[:10]  # 10 meilleurs r√©sultats\n",
    "    else:\n",
    "        all_scores = []\n",
    "        for table in tables:\n",
    "            documents = data[table]\n",
    "            vectorizer = vectorizers[table]\n",
    "            tfidf_matrix = vectorizer.transform([concatenate_row_values(row) for row in documents])\n",
    "            query_vec = vectorizer.transform([query])\n",
    "            scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "            all_scores.extend([(score, row, table) for score, row in zip(scores, documents)])\n",
    "        return sorted(all_scores, key=lambda x: x[0], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la connexion √† la base de donn√©es et du metadata\n",
    "engine = create_engine(\"mysql+pymysql://root:root@9mois.ownedge.fr:3306/9mois\")\n",
    "metadata = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chargement des stop words\n",
    "stop_words = load_stop_words(\"stop_words_french.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Traitement des tables et consolidation des donn√©es\n",
    "tables = ['articles', 'food', 'questions', 'recipes']\n",
    "data = {table: process_table(engine, metadata, table) for table in tables}\n",
    "\n",
    "# Pr√©paration du vectorisateur TF-IDF\n",
    "vectorizers = {table: TfidfVectorizer(stop_words=stop_words) for table in tables}\n",
    "for table in tables:\n",
    "    docs = [concatenate_row_values(row) for row in data[table]]\n",
    "    vectorizers[table].fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5577068863160419,\n",
       "  (8, '# Le kiwi, atout vitamin√© de votre grossesse', \"## ***ü•ùLe kiwi,\\xa0une bombe de vitamines et de fibres\\xa0!***\\n\\nLe **kiwi** est un fruit star du mois de janvier. Avec son go√ªt acidul√©, il\\xa0chan ... (1148 characters truncated) ... our l'instant, peu d'√©tudes\\xa0sur le sujet, ce serait certains polyph√©nols (des petites mol√©cules anti-oxydantes) qui produiraient un effet laxatif.\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 10, 22, 11), 0, datetime.date(2023, 6, 13), '/images/articles/le-kiwi-atout-vitamine-de-votre-grossesse.jpeg', 1),\n",
       "  'articles'),\n",
       " (0.3939051836558003,\n",
       "  (28, 'Brochettes de cabillaud sauce kiwi', 30, 'Facile', 'Co√ªt Moyen', '/images/recipes/brochette_cabillaud_sauce_kiwi.png', '> ***L‚Äôavis 9 mois √† croquer :***\\n> \\n> - Une recette qui contribue √† la **consommation hebdomadaire de poisson blanc** et qui apporte **des vitamin ... (307 characters truncated) ... aud** peut √™tre remplac√© par d‚Äôautres poissons blanc comme le **colin moins co√ªteux** ! Si on a un petit budget, on peut le trouver surgel√© en filet.', 4, '*Liste non-exhaustive (de nouvelles id√©es viendront s‚Äôajouter avec le temps)*\\n\\n- Brochettes de cabillaud sauce kiwi + riz (complet de pr√©f√©rence)\\n ... (29 characters truncated) ... ce kiwi + salade + pain (complet de pr√©f√©rence)\\n- Brochettes de cabillaud sauce kiwi + tagliatelles\\n- Brochettes de cabillaud sauce kiwi + boulgour', '1. Pr√©chauffer le four √† 200¬∞C.\\n2. Couper le dos de cabillaud en petits d√©s (environ 2 cm de large, ce qui fait environ la moiti√© d‚Äôun kiwi). \\n3. E ... (487 characters truncated) ...  demi-citron (vert).\\n    2. Saler et poivrer.\\n8. Servir les brochettes de cabillaud arros√©es de la sauce kiwi avec quelques tranches de kiwi frais.', '## Ingr√©dients\\n\\n- [ ] 600g de dos de cabillaud (*ou filets de colin*)\\n- [ ] 4 cuill√®re √† soupe de graines de s√©same\\n- [ ] 5 kiwis\\n- [ ] 3 c. √† soupe d‚Äôhuile d‚Äôolive\\n- [ ] 1/2 citron (*vert pour un go√ªt plus acidul√©*)\\n- [ ] Sel\\n- [ ] Poivre', 0, datetime.date(2023, 6, 10), datetime.datetime(2023, 6, 10, 15, 31, 36), datetime.datetime(2023, 6, 10, 15, 31, 36)),\n",
       "  'recipes'),\n",
       " (0.327488636805234,\n",
       "  (18, 'Trifle kiwi, muesli et fromage blanc', 10, 'Facile', 'Bon March√©', '/images/recipes/Trifle_kiwi_fromageblanc_muesli.png', '> ***L‚Äôavis 9 mois √† croquer :***\\n> \\n> - Cette recette peut √™tre consomm√©e pour un **petit-d√©jeuner √©quilibr√©,** elle comporte un **fruit** (le kiw ... (343 characters truncated) ...  la recette sont donn√©es pour une **portion au petit-d√©jeuner**. Dans le cas d‚Äôun dessert, on peut diviser les quantit√©s par deux pour mieux dig√©rer.', 1, '', '- Version pour une jolie pr√©sentation :\\n    1. Mettre 2 cuill√®res √† soupe de muesli au fond d‚Äôun verre. \\n    2. Couper un kiwi en tranches et dispo ... (230 characters truncated) ... rer :\\n    1. Couper les kiwis en d√©s\\n    2. Dans un verre, alterner une couche de muesli, une couche de fromage blanc et une couche de d√©s de kiwi.', '## Ingr√©dients\\n\\n- [ ] 2 kiwis (*1 pour les petites faims*)\\n- [ ] 150g de fromage blanc (*100g pour les petites faims*)\\n- [ ] 40g de muesli aux noix (*25g pour les petites faims*)', 0, datetime.date(2023, 6, 10), datetime.datetime(2023, 6, 10, 15, 31, 36), datetime.datetime(2023, 6, 10, 15, 31, 36)),\n",
       "  'recipes'),\n",
       " (0.14451081868654655,\n",
       "  (16, 'Sorbet kiwi aux amandes', 380, 'Facile', 'Bon March√©', '/images/recipes/Sorbet_kiwi_amandes.png', '> ***L‚Äôavis 9 mois √† croquer :***\\n> \\n> - Ce sorbet est tr√®s fruit√© et **riche en vitamine C** (2 boules couvrent 80% des apports recommand√©s en vit ... (19 characters truncated) ... ssesse).\\n> - Agr√©ment√© d‚Äôamandes effil√©es ou d‚Äô√©clats de biscuits, il peut devenir un **dessert frais**, **parfait pour terminer un repas copieux**.', 4, '.', '1. Dans une casserole, faire un sirop en faisant bouillir le sucre et l‚Äôeau (le m√©lange ne doit pas colorer, il doit simplement √©paissir un peu). Lai ... (836 characters truncated) ...  colorent.\\n9. Faire des boules de sorbet, les disposer dans les coques de peau de kiwi (ou dans des coupes) et parsemer d‚Äôamandes effil√©es grill√©es.', '## Ingr√©dients\\n\\n- [ ] 1kg de kiwis (environ 10-12)\\n- [ ] 100g de sucre\\n- [ ] 300ml d‚Äôeau\\n- [ ] 50g d‚Äôamandes effil√©es', 0, datetime.date(2023, 6, 10), datetime.datetime(2023, 6, 10, 15, 31, 36), datetime.datetime(2023, 7, 1, 17, 31, 26)),\n",
       "  'recipes'),\n",
       " (0.10155304969939744,\n",
       "  (30, 'Je ne mange pas (ou peu) de viande, comment consommer assez de fer ?', \"Les **l√©gumineuses** (*pois chiches, lentilles, pois cass√©s, etc*.) sont de bonnes sources de **fer** Les noisettes, les amandes et le chocolat noir  ... (741 characters truncated) ...  les nutriments qui ne se retrouvent que dans les produits animaux (vitamine B12, vitamine D, fer, zinc, voir calcium dans le cas d'un r√©gime vegan).\", 'Fait', 0, None, 1, datetime.datetime(2023, 6, 12, 13, 36, 50), datetime.datetime(2023, 6, 13, 7, 45, 5)),\n",
       "  'questions'),\n",
       " (0.04359772712889292,\n",
       "  (9, '# Les lentilles, sources de fer mais pas seulement‚Ä¶', \"## ***Les lentilles,\\xa0grand b√©n√©fice et petit prix !***\\n<br><br>\\n![image.png](/images/articles/Les_lentilles__sources_de_fer_mais_pas_seulement_f ... (2621 characters truncated) ... grette aux agrumes.**\\n- Apr√®s un\\xa0**plat chaud avec des\\xa0lentilles**, consommer un fruit riche en vitamine C :\\xa0**kiwi, orange, mandarine**...\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 10, 23, 27), 0, datetime.date(2023, 6, 13), '/images/articles/les-lentilles-sources-de-fer-mais-pas-seulement.jpeg', 1),\n",
       "  'articles'),\n",
       " (0.03871406721919865,\n",
       "  (43, 'Mon assiette √©quilibr√©e √† petit budget !', 'ü•ó***Il n‚Äôest pas toujours √©vident d‚Äô√©quilibrer son assiette !*** ***Mais c‚Äôest possible, m√™me avec un petit budget et quoi de mieux qu‚Äôune image pour ... (2388 characters truncated) ... br><br>\\n\\n\\nüõí**Alors, vous avez pu faire votre liste de courses ?** <br>\\n**N‚Äôh√©sitez pas √† consulter les recettes ci-dessous pour vous inspirer !**', datetime.datetime(2023, 9, 8, 18, 40, 6), datetime.datetime(2023, 9, 8, 19, 5, 37), 0, datetime.date(2023, 9, 8), '/images/articles/Assiette_equilibree_budget.png', 1),\n",
       "  'articles'),\n",
       " (0.019738490252706638,\n",
       "  (35, '#TOP 10 des aliments √† avoir dans son placard !', '## Nutrition, petits prix et pas de cuisine \\n\\nPendant la grossesse, le corps a besoin d\\'une plus grande quantit√© de nutriments, notamment de vitam ... (6018 characters truncated) ... oisacroquer.fr/recipes/25)</u>**<br>\\n***A noter*** : l\\'huile de colza peut √™tre l√©g√®rement chauff√©e mais ne peut pas √™tre utilis√©e pour la friture.', datetime.datetime(2023, 6, 19, 8, 12, 6), datetime.datetime(2023, 6, 19, 8, 25, 9), 0, datetime.date(2023, 6, 19), '/images/articles/plats_vegan.jpg', 1),\n",
       "  'articles'),\n",
       " (0.0,\n",
       "  (1, '# Comment g√©rer les remont√©es acides et br√ªlures d‚Äôestomac ?', \"## Des astuces simples tout au long de la journ√©e\\n\\nEnceinte, quand on\\xa0souffre\\xa0de\\xa0**remont√©es acides**\\xa0et de\\xa0**br√ªlures d'estomac**,  ... (1910 characters truncated) ... e montrent r√©ellement. Les remont√©es acides\\xa0sont principalement dues √† des modifications hormonales (augmentation de la progest√©rone).\\n\\n</aside>\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 9, 28, 57), 0, datetime.date(2023, 6, 11), '/images/articles/comment-gerer-les-remontees-acides-et-brulures-destomac.jpeg', 1),\n",
       "  'articles'),\n",
       " (0.0,\n",
       "  (2, '# Comment (assez) s‚Äôhydrater enceinte?', \"## ***Boire\\xa0assez d'eau\\xa0:\\xa0un r√©flexe sant√© pour maman et b√©b√©\\xa0!***\\n<br><br>\\nPendant la grossesse, le **volume d'eau dans le corps augme ... (4433 characters truncated) ... in√©rales naturelles et de source.\\n\\nLorsque b√©b√© sera n√©, pensez √† bien v√©rifier l'√©tiquette des eaux en bouteille, si vous souhaitez lui en donner.\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 9, 30, 15), 0, datetime.date(2023, 6, 12), '/images/articles/comment-assez-shydrater-enceinte.jpeg', 1),\n",
       "  'articles')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('kiwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'get_document_id() takes 1 positional argument but 2 were given'} 500\n"
     ]
    }
   ],
   "source": [
    "query = 'kiwi'\n",
    "table_choices = 'articles'\n",
    "\n",
    "if not query:\n",
    "   print(({'error': 'Aucune requ√™te fournie.'}), 400)\n",
    "\n",
    "try:\n",
    "    if table_choices != 'Toutes':\n",
    "        table_choices = table_choices.split(',')  # S√©pare les noms de tables si plusieurs sont fournis\n",
    "        all_scores = []\n",
    "        for table_choice in table_choices:\n",
    "            if table_choice in tables:\n",
    "                table_results = search(query, table_choice)\n",
    "                for score, row in table_results:\n",
    "                    doc_id = get_document_id(row, table_choice)\n",
    "                    all_scores.append((score, row, table_choice, doc_id))\n",
    "        all_scores = sorted(all_scores, key=lambda x: x[0], reverse=True)[:10]\n",
    "    else:\n",
    "        all_scores = search(query, table_choices)\n",
    "    \n",
    "    \n",
    "    formatted_results = [{\n",
    "        'score': score,\n",
    "        'document_id': doc_id,\n",
    "        'document': concatenate_row_values(row),\n",
    "        'table': table\n",
    "    } for score, row, table, doc_id in all_scores]\n",
    "\n",
    "    print(({\n",
    "        'query': query,\n",
    "        'table': table_choices,\n",
    "        'results': formatted_results\n",
    "    }))\n",
    "except Exception as e:\n",
    "    print(({'error': str(e)}), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search('kiwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"j'ai des naus√©es et je manque de f\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = SnowballStemmer('french')\n",
    "stemmer.stem(\"j'ai des naus√©es et je manque de fer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['jai', 'des' , 'naus√©es', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_test = TfidfVectorizer()\n",
    "\n",
    "analyzer_test = vect_test.build_analyzer()\n",
    "preprocess_test = vect_test.build_preprocessor()\n",
    "token_test = vect_test.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j'ai des naus√©es et je manque de fer\n",
      "['ai', 'des', 'naus√©es', 'et', 'je', 'manque', 'de', 'fer']\n",
      "['ai', 'de', 'naus', 'et', 'je', 'manqu', 'de', 'fer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ai de naus et je manqu de fer'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_test = \"j'ai des naus√©es et je manque de fer\"\n",
    "\n",
    "# step_1 = analyzer_test(string_test)\n",
    "# print(step_1)\n",
    "step_2 = preprocess_test(string_test)\n",
    "print(step_2)\n",
    "step_3 = token_test(step_2)\n",
    "print(step_3)\n",
    "step_4 = []\n",
    "for words in step_3:\n",
    "    stemmed_word = stemmer.stem(words)\n",
    "    step_4.append(stemmed_word)\n",
    "print(step_4)\n",
    "' '.join(step_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai', 'des', 'naus√©es', 'et', 'je', 'manque', 'de', 'fer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_test(string_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aidesnaus√©esetjemanquedefer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_list = ['ai', 'des', 'naus√©es', 'et', 'je', 'manque', 'de', 'fer']\n",
    "concatenate_row_values(mock_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_maison(text):\n",
    "    vect_test = TfidfVectorizer()\n",
    "    token_test = vect_test.build_tokenizer()\n",
    "    tokenized_text = token_test(text)\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    output = [stemmer.stem(word) for word in tokenized_text]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vect_maison = TfidfVectorizer(analyzer=analyzer_maison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_vect_maison\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1361\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;66;03m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;66;03m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# TfidfVectorizer.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "test_vect_maison.fit_transform(string_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stemmer = SnowballStemmer(\"french\")\n",
    "    tokens = word_tokenize(text, language=\"french\")\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Fonction pour traiter une table\n",
    "def process_table(engine, metadata, table_name):\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    with engine.connect() as connection:\n",
    "        result_set = connection.execute(table.select()).fetchall()\n",
    "    return result_set\n",
    "\n",
    "# Pr√©paration du vectorisateur TF-IDF avec la fonction de pr√©traitement\n",
    "vectorizers = {table: TfidfVectorizer(stop_words=stop_words, preprocessor=preprocess_text, tokenizer=word_tokenize) for table in tables}\n",
    "for table in tables:\n",
    "    docs = [' '.join(str(value) for value in row) for row in data[table]]\n",
    "    vectorizers[table].fit_transform(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
