{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def process_table(engine, metadata, table_name):\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    with engine.connect() as connection:\n",
    "        result_set = connection.execute(table.select()).fetchall()\n",
    "    return result_set\n",
    "\n",
    "def concatenate_row_values(row):\n",
    "    return ''.join(str(value) for value in row if isinstance(value, str))    \n",
    "\n",
    "def get_document_id(doc):\n",
    "    return doc[0]\n",
    "\n",
    "def search(query, table_name= 'Toutes'):\n",
    "    if table_name != \"Toutes\":\n",
    "        documents = data[table_name]\n",
    "        vectorizer = vectorizers[table_name]\n",
    "        tfidf_matrix = vectorizer.transform([concatenate_row_values(row) for row in documents])\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "        ranked_scores = sorted([(score, row) for score, row in zip(scores, documents)], reverse=True, key=lambda x: x[0])\n",
    "        return ranked_scores[:10]  # 10 meilleurs résultats\n",
    "    else:\n",
    "        all_scores = []\n",
    "        for table in tables:\n",
    "            documents = data[table]\n",
    "            vectorizer = vectorizers[table]\n",
    "            tfidf_matrix = vectorizer.transform([concatenate_row_values(row) for row in documents])\n",
    "            query_vec = vectorizer.transform([query])\n",
    "            scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "            all_scores.extend([(score, row, table) for score, row in zip(scores, documents)])\n",
    "        return sorted(all_scores, key=lambda x: x[0], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la connexion à la base de données et du metadata\n",
    "engine = create_engine(\"mysql+pymysql://root:root@9mois.ownedge.fr:3306/9mois\")\n",
    "metadata = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chargement des stop words\n",
    "stop_words = load_stop_words(\"stop_words_french.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Traitement des tables et consolidation des données\n",
    "tables = ['articles', 'food', 'questions', 'recipes']\n",
    "data = {table: process_table(engine, metadata, table) for table in tables}\n",
    "\n",
    "# Préparation du vectorisateur TF-IDF\n",
    "vectorizers = {table: TfidfVectorizer(stop_words=stop_words) for table in tables}\n",
    "for table in tables:\n",
    "    docs = [concatenate_row_values(row) for row in data[table]]\n",
    "    vectorizers[table].fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5577068863160419,\n",
       "  (8, '# Le kiwi, atout vitaminé de votre grossesse', \"## ***🥝Le kiwi,\\xa0une bombe de vitamines et de fibres\\xa0!***\\n\\nLe **kiwi** est un fruit star du mois de janvier. Avec son goût acidulé, il\\xa0chan ... (1148 characters truncated) ... our l'instant, peu d'études\\xa0sur le sujet, ce serait certains polyphénols (des petites molécules anti-oxydantes) qui produiraient un effet laxatif.\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 10, 22, 11), 0, datetime.date(2023, 6, 13), '/images/articles/le-kiwi-atout-vitamine-de-votre-grossesse.jpeg', 1),\n",
       "  'articles'),\n",
       " (0.3939051836558003,\n",
       "  (28, 'Brochettes de cabillaud sauce kiwi', 30, 'Facile', 'Coût Moyen', '/images/recipes/brochette_cabillaud_sauce_kiwi.png', '> ***L’avis 9 mois à croquer :***\\n> \\n> - Une recette qui contribue à la **consommation hebdomadaire de poisson blanc** et qui apporte **des vitamin ... (307 characters truncated) ... aud** peut être remplacé par d’autres poissons blanc comme le **colin moins coûteux** ! Si on a un petit budget, on peut le trouver surgelé en filet.', 4, '*Liste non-exhaustive (de nouvelles idées viendront s’ajouter avec le temps)*\\n\\n- Brochettes de cabillaud sauce kiwi + riz (complet de préférence)\\n ... (29 characters truncated) ... ce kiwi + salade + pain (complet de préférence)\\n- Brochettes de cabillaud sauce kiwi + tagliatelles\\n- Brochettes de cabillaud sauce kiwi + boulgour', '1. Préchauffer le four à 200°C.\\n2. Couper le dos de cabillaud en petits dés (environ 2 cm de large, ce qui fait environ la moitié d’un kiwi). \\n3. E ... (487 characters truncated) ...  demi-citron (vert).\\n    2. Saler et poivrer.\\n8. Servir les brochettes de cabillaud arrosées de la sauce kiwi avec quelques tranches de kiwi frais.', '## Ingrédients\\n\\n- [ ] 600g de dos de cabillaud (*ou filets de colin*)\\n- [ ] 4 cuillère à soupe de graines de sésame\\n- [ ] 5 kiwis\\n- [ ] 3 c. à soupe d’huile d’olive\\n- [ ] 1/2 citron (*vert pour un goût plus acidulé*)\\n- [ ] Sel\\n- [ ] Poivre', 0, datetime.date(2023, 6, 10), datetime.datetime(2023, 6, 10, 15, 31, 36), datetime.datetime(2023, 6, 10, 15, 31, 36)),\n",
       "  'recipes'),\n",
       " (0.327488636805234,\n",
       "  (18, 'Trifle kiwi, muesli et fromage blanc', 10, 'Facile', 'Bon Marché', '/images/recipes/Trifle_kiwi_fromageblanc_muesli.png', '> ***L’avis 9 mois à croquer :***\\n> \\n> - Cette recette peut être consommée pour un **petit-déjeuner équilibré,** elle comporte un **fruit** (le kiw ... (343 characters truncated) ...  la recette sont données pour une **portion au petit-déjeuner**. Dans le cas d’un dessert, on peut diviser les quantités par deux pour mieux digérer.', 1, '', '- Version pour une jolie présentation :\\n    1. Mettre 2 cuillères à soupe de muesli au fond d’un verre. \\n    2. Couper un kiwi en tranches et dispo ... (230 characters truncated) ... rer :\\n    1. Couper les kiwis en dés\\n    2. Dans un verre, alterner une couche de muesli, une couche de fromage blanc et une couche de dés de kiwi.', '## Ingrédients\\n\\n- [ ] 2 kiwis (*1 pour les petites faims*)\\n- [ ] 150g de fromage blanc (*100g pour les petites faims*)\\n- [ ] 40g de muesli aux noix (*25g pour les petites faims*)', 0, datetime.date(2023, 6, 10), datetime.datetime(2023, 6, 10, 15, 31, 36), datetime.datetime(2023, 6, 10, 15, 31, 36)),\n",
       "  'recipes'),\n",
       " (0.14451081868654655,\n",
       "  (16, 'Sorbet kiwi aux amandes', 380, 'Facile', 'Bon Marché', '/images/recipes/Sorbet_kiwi_amandes.png', '> ***L’avis 9 mois à croquer :***\\n> \\n> - Ce sorbet est très fruité et **riche en vitamine C** (2 boules couvrent 80% des apports recommandés en vit ... (19 characters truncated) ... ssesse).\\n> - Agrémenté d’amandes effilées ou d’éclats de biscuits, il peut devenir un **dessert frais**, **parfait pour terminer un repas copieux**.', 4, '.', '1. Dans une casserole, faire un sirop en faisant bouillir le sucre et l’eau (le mélange ne doit pas colorer, il doit simplement épaissir un peu). Lai ... (836 characters truncated) ...  colorent.\\n9. Faire des boules de sorbet, les disposer dans les coques de peau de kiwi (ou dans des coupes) et parsemer d’amandes effilées grillées.', '## Ingrédients\\n\\n- [ ] 1kg de kiwis (environ 10-12)\\n- [ ] 100g de sucre\\n- [ ] 300ml d’eau\\n- [ ] 50g d’amandes effilées', 0, datetime.date(2023, 6, 10), datetime.datetime(2023, 6, 10, 15, 31, 36), datetime.datetime(2023, 7, 1, 17, 31, 26)),\n",
       "  'recipes'),\n",
       " (0.10155304969939744,\n",
       "  (30, 'Je ne mange pas (ou peu) de viande, comment consommer assez de fer ?', \"Les **légumineuses** (*pois chiches, lentilles, pois cassés, etc*.) sont de bonnes sources de **fer** Les noisettes, les amandes et le chocolat noir  ... (741 characters truncated) ...  les nutriments qui ne se retrouvent que dans les produits animaux (vitamine B12, vitamine D, fer, zinc, voir calcium dans le cas d'un régime vegan).\", 'Fait', 0, None, 1, datetime.datetime(2023, 6, 12, 13, 36, 50), datetime.datetime(2023, 6, 13, 7, 45, 5)),\n",
       "  'questions'),\n",
       " (0.04359772712889292,\n",
       "  (9, '# Les lentilles, sources de fer mais pas seulement…', \"## ***Les lentilles,\\xa0grand bénéfice et petit prix !***\\n<br><br>\\n![image.png](/images/articles/Les_lentilles__sources_de_fer_mais_pas_seulement_f ... (2621 characters truncated) ... grette aux agrumes.**\\n- Après un\\xa0**plat chaud avec des\\xa0lentilles**, consommer un fruit riche en vitamine C :\\xa0**kiwi, orange, mandarine**...\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 10, 23, 27), 0, datetime.date(2023, 6, 13), '/images/articles/les-lentilles-sources-de-fer-mais-pas-seulement.jpeg', 1),\n",
       "  'articles'),\n",
       " (0.03871406721919865,\n",
       "  (43, 'Mon assiette équilibrée à petit budget !', '🥗***Il n’est pas toujours évident d’équilibrer son assiette !*** ***Mais c’est possible, même avec un petit budget et quoi de mieux qu’une image pour ... (2388 characters truncated) ... br><br>\\n\\n\\n🛒**Alors, vous avez pu faire votre liste de courses ?** <br>\\n**N’hésitez pas à consulter les recettes ci-dessous pour vous inspirer !**', datetime.datetime(2023, 9, 8, 18, 40, 6), datetime.datetime(2023, 9, 8, 19, 5, 37), 0, datetime.date(2023, 9, 8), '/images/articles/Assiette_equilibree_budget.png', 1),\n",
       "  'articles'),\n",
       " (0.019738490252706638,\n",
       "  (35, '#TOP 10 des aliments à avoir dans son placard !', '## Nutrition, petits prix et pas de cuisine \\n\\nPendant la grossesse, le corps a besoin d\\'une plus grande quantité de nutriments, notamment de vitam ... (6018 characters truncated) ... oisacroquer.fr/recipes/25)</u>**<br>\\n***A noter*** : l\\'huile de colza peut être légèrement chauffée mais ne peut pas être utilisée pour la friture.', datetime.datetime(2023, 6, 19, 8, 12, 6), datetime.datetime(2023, 6, 19, 8, 25, 9), 0, datetime.date(2023, 6, 19), '/images/articles/plats_vegan.jpg', 1),\n",
       "  'articles'),\n",
       " (0.0,\n",
       "  (1, '# Comment gérer les remontées acides et brûlures d’estomac ?', \"## Des astuces simples tout au long de la journée\\n\\nEnceinte, quand on\\xa0souffre\\xa0de\\xa0**remontées acides**\\xa0et de\\xa0**brûlures d'estomac**,  ... (1910 characters truncated) ... e montrent réellement. Les remontées acides\\xa0sont principalement dues à des modifications hormonales (augmentation de la progestérone).\\n\\n</aside>\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 9, 28, 57), 0, datetime.date(2023, 6, 11), '/images/articles/comment-gerer-les-remontees-acides-et-brulures-destomac.jpeg', 1),\n",
       "  'articles'),\n",
       " (0.0,\n",
       "  (2, '# Comment (assez) s’hydrater enceinte?', \"## ***Boire\\xa0assez d'eau\\xa0:\\xa0un réflexe santé pour maman et bébé\\xa0!***\\n<br><br>\\nPendant la grossesse, le **volume d'eau dans le corps augme ... (4433 characters truncated) ... inérales naturelles et de source.\\n\\nLorsque bébé sera né, pensez à bien vérifier l'étiquette des eaux en bouteille, si vous souhaitez lui en donner.\", datetime.datetime(2023, 6, 10, 15, 31, 38), datetime.datetime(2023, 6, 13, 9, 30, 15), 0, datetime.date(2023, 6, 12), '/images/articles/comment-assez-shydrater-enceinte.jpeg', 1),\n",
       "  'articles')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('kiwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'get_document_id() takes 1 positional argument but 2 were given'} 500\n"
     ]
    }
   ],
   "source": [
    "query = 'kiwi'\n",
    "table_choices = 'articles'\n",
    "\n",
    "if not query:\n",
    "   print(({'error': 'Aucune requête fournie.'}), 400)\n",
    "\n",
    "try:\n",
    "    if table_choices != 'Toutes':\n",
    "        table_choices = table_choices.split(',')  # Sépare les noms de tables si plusieurs sont fournis\n",
    "        all_scores = []\n",
    "        for table_choice in table_choices:\n",
    "            if table_choice in tables:\n",
    "                table_results = search(query, table_choice)\n",
    "                for score, row in table_results:\n",
    "                    doc_id = get_document_id(row, table_choice)\n",
    "                    all_scores.append((score, row, table_choice, doc_id))\n",
    "        all_scores = sorted(all_scores, key=lambda x: x[0], reverse=True)[:10]\n",
    "    else:\n",
    "        all_scores = search(query, table_choices)\n",
    "    \n",
    "    \n",
    "    formatted_results = [{\n",
    "        'score': score,\n",
    "        'document_id': doc_id,\n",
    "        'document': concatenate_row_values(row),\n",
    "        'table': table\n",
    "    } for score, row, table, doc_id in all_scores]\n",
    "\n",
    "    print(({\n",
    "        'query': query,\n",
    "        'table': table_choices,\n",
    "        'results': formatted_results\n",
    "    }))\n",
    "except Exception as e:\n",
    "    print(({'error': str(e)}), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search('kiwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"j'ai des nausées et je manque de f\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = SnowballStemmer('french')\n",
    "stemmer.stem(\"j'ai des nausées et je manque de fer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['jai', 'des' , 'nausées', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_test = TfidfVectorizer()\n",
    "\n",
    "analyzer_test = vect_test.build_analyzer()\n",
    "preprocess_test = vect_test.build_preprocessor()\n",
    "token_test = vect_test.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j'ai des nausées et je manque de fer\n",
      "['ai', 'des', 'nausées', 'et', 'je', 'manque', 'de', 'fer']\n",
      "['ai', 'de', 'naus', 'et', 'je', 'manqu', 'de', 'fer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ai de naus et je manqu de fer'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_test = \"j'ai des nausées et je manque de fer\"\n",
    "\n",
    "# step_1 = analyzer_test(string_test)\n",
    "# print(step_1)\n",
    "step_2 = preprocess_test(string_test)\n",
    "print(step_2)\n",
    "step_3 = token_test(step_2)\n",
    "print(step_3)\n",
    "step_4 = []\n",
    "for words in step_3:\n",
    "    stemmed_word = stemmer.stem(words)\n",
    "    step_4.append(stemmed_word)\n",
    "print(step_4)\n",
    "' '.join(step_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai', 'des', 'nausées', 'et', 'je', 'manque', 'de', 'fer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_test(string_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aidesnauséesetjemanquedefer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_list = ['ai', 'des', 'nausées', 'et', 'je', 'manque', 'de', 'fer']\n",
    "concatenate_row_values(mock_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_maison(text):\n",
    "    vect_test = TfidfVectorizer()\n",
    "    token_test = vect_test.build_tokenizer()\n",
    "    tokenized_text = token_test(text)\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    output = [stemmer.stem(word) for word in tokenized_text]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vect_maison = TfidfVectorizer(analyzer=analyzer_maison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_vect_maison\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thaz\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1361\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;66;03m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;66;03m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# TfidfVectorizer.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "test_vect_maison.fit_transform(string_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stemmer = SnowballStemmer(\"french\")\n",
    "    tokens = word_tokenize(text, language=\"french\")\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Fonction pour traiter une table\n",
    "def process_table(engine, metadata, table_name):\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    with engine.connect() as connection:\n",
    "        result_set = connection.execute(table.select()).fetchall()\n",
    "    return result_set\n",
    "\n",
    "# Préparation du vectorisateur TF-IDF avec la fonction de prétraitement\n",
    "vectorizers = {table: TfidfVectorizer(stop_words=stop_words, preprocessor=preprocess_text, tokenizer=word_tokenize) for table in tables}\n",
    "for table in tables:\n",
    "    docs = [' '.join(str(value) for value in row) for row in data[table]]\n",
    "    vectorizers[table].fit_transform(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
